
# Decision-Aware Stochastic Forecasting for Small High-cost Inventory Under Covariate Scarcity


####    Jithakrishna Prakash   |   jprakashoff@gmail.com   |   [linkedin.com/in/jithakrishna-prakash/](https://www.linkedin.com/in/jithakrishna-prakash/)

***

This project involves forecasting demand for small inventory data using stochastic consumption patterns under lack of covariates. Stochastic modeling of demand was performed using **Poisson-Gamma Conjugate Model** Mock inventory data inspired from real-world biotech QC inventories is used in this analysis. Sensitive information such as material name and expiries have been masked.

The potential benefits of this analysis were:

1. Optimization of restock frequency
1. Reduction of effort to perform frequent restocks
1. Optimum stock level that maximizes inventory level as well as minimize expected waste


# Stochastic Process

Bayesian analysis involves representing a stochastic process (random process) using known probability distributions called likelihood functions. Typically, these likelihood functions will need parameter values to work. We usually start with a range of values, or a probability distribution, for these parameter(s), called priors. Then we will obtain a new distribution called the posterior probability distribution by combining the likelihood and the prior based on the Bayes Theorem. The posterior probability distribution will then be representative of the possible values of the random variable ranked by plausibility.

In this case, the nature of my inventory data is **Count Data**. Count data is fundamentally modeled using the **Poisson Distribution** as the likelihood function:

$P(x~|~\lambda) = \frac{\lambda^x} {x!} ~ e^{-\lambda}$

where $x$ is the count for one time period, $\lambda$ is the rate for one time period. In this problem, $\lambda$ is the daily usage which is assumed to be constant for a given week and the counts produced are weekly. 

However, the underlying usage changes every week. So, $\lambda$ is assumed to follow the **Gamma Distribution**:

$p(\lambda~|~shape = a,~rate = b) = \frac {b^a} {\Gamma(a)} ~ \lambda^{(a~-~1)} ~ e^{-b\lambda}$

where $a$ is the shape parameter and $b$ is the rate parameter.

Here, the Gamma Distribution is the **prior** we assume for the lambda parameter of the Poisson likelihood.

With a Poisson Likelihood and Gamma Prior we obtain a **closed-form expression** for the **Posterior Predictive Distribution** of consumption $Y$, which is the **Negative Binomial Distribution**: 

$Y \sim NegBinom(x~|~size = a,~prob = \frac {b} {1 ~+~ b})$ 

This is the posterior probability distribution for one occurrence in the time scale of $\lambda$, which is **one day**. We assume that for $H$ days, parameters $a$ and $b$ of the gamma prior do not change. Following this, to forecast consumption over a horizon of $H$ days, we exploit a key property of Poisson processes: **counts scale linearly with exposure time**. If daily usage follows a Poisson process with rate $\lambda$, then the total count over $H$ days follows a Poisson distribution with mean $\lambda \cdot H$. However, the variability of $\lambda$ is already captured by the gamma prior. 

This allows the posterior distribution of consumption to be extended to $H$ days by: 

$Y_{future} \sim NegBinom(x ~|~ size = a,~prob = \frac{b} {H ~+~ b})$

We use this posterior predictive distribution to model consumption for $H$ days.

Since there is a closed-form expression for the posterior probability distribution, the Gamma distribution is called the **Conjugate Prior** of the Poisson Distribution.

For the priors for $a$ and $b$, we will derive empirical Bayes priors from the data, which are simply $Median(days~per~observation)$ for $b$ and $\frac {Median(weekly~consumption)} {Prior(b)}$ for $a$. 

Due to the posterior having a closed-form expression, the posterior update reduces into **simple parameter update rules**:  
Gamma Parameters:  
$a = a_{prior} ~+~ \Sigma(consumption)$  
$b = b_{prior} ~+~ \Sigma(days)$  
  
Negative Binomial Posterior Predictive Parameters:  
  
$size = a$  
$prob = \frac {b} {b ~+~ Forecast~Days~(H)}$  

The Negative Binomial distribution exhibits properties such as right-tailed distribution, overdispersion in the random variable (variance is greater than the mean), and the variance and mean have a quadratic relationship.

The next section is the exploratory data analysis to assess the suitability of the Poisson-Gamma Mix Model for this dataset.

# Libraries

```{r}

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)

```


```{r, echo=TRUE}

library(readxl)
library(dplyr)
library(ggplot2)
library(lubridate)
library(stats)
library(patchwork)
library(geomtextpath)

source("utils.R")

set.seed(42)

```


# Exploratory Data Analysis

The raw data contains mock weekly stock counts that mimic inventory data inspired from a real world biotech QC environment. The materials are high cost and have short expiry windows.

```{r}

import_data <- readxl::read_excel("./UnitsUsed.xlsx", sheet=1)

import_data$Material <- as.factor(import_data$Material)
import_data$Date <- as.Date(import_data$Date, format="%m/%d/%Y")

import_data <- import_data%>%
  rename(date = Date)%>%
  rename(material = Material)%>%
  rename(n = `Days Since Last Inventory`)%>%
  rename(y = `Units Used in Week`)%>%
  rename(stock = Stock)%>%
  rename(rolling_y_in_cycle = `Total Used since last restock`)%>%
  rename(rolling_n_in_cycle = `Days since last restock`)%>%
  rename(total_y_prev_cycle = `Previous Cycle Total Use`)%>%
  rename(total_n_prev_cycle = `Previous Cycle Total Days`)

```

```{r, echo=TRUE}

head(import_data)

```


The raw data originally contained only the Material Name, Weekly Stock Count, and Expiration Date (not shown). Expiration dates are hidden to respect data privacy.

However, to make any forecast, the units used in week is required. 
For this, an **external Excel VBA script** written externally that calculates the weekly units used **y** for all materials using combinations of checks such as:
1. Was restock performed in given week?
2. Is there expiry information available for the material in given week and previous week?
3. Did any units expire in previous week?

From this, all the other metrics in the data were calculated.


## Data Visualization

```{r}

hist <- c(); cycle_lengths <- c()

for(m in unique(import_data$material)) {
  
  hist <- c(hist, 
            ggplot(data=import_data%>%filter(material == m), aes(x=y)) +
            geom_histogram(aes(y=after_stat(density)), bins=20, fill="grey", color = "black") +
            geom_density(alpha=0.2, fill="red") +
            xlab("Units used per week") +
            labs(title=paste0(m, ": Units used per week")) +
            theme_minimal())
  
  cycle_lengths <- c(cycle_lengths, 
                     ggplot(data=import_data%>%filter(material == m), 
                          aes(x=total_n_prev_cycle, y=total_y_prev_cycle)) +
                          geom_point() +
                          geom_smooth(method="lm") +
                          xlab("Days in cycle") +
                          ylab("Total Units used in cycle") +
                          labs(title = paste0(m, ": Total used per stock cycle")) +
                          theme_minimal())
  
}

```

```{r, message=FALSE}

patchwork::wrap_plots(hist[[1]], cycle_lengths[[1]], hist[[2]], cycle_lengths[[2]], ncol=2, nrow=2)

```

The distribution of weekly use is right-tailed for both materials. There also is a positive correlation between restock cycle length (weeks) and the total number of units used in a given cycle. This indicated that the idea of forecasting usage across a specified time horizon could be plausible. 

## Mean-Variance analysis of rolling windows

For forecasting, rolling window analysis is a powerful method that can reveal trends in the data. Here, the drift of mean and the variance of different rolling windows are analyzed. 



```{r, warning=FALSE}

for(m in unique(import_data$material)) {
  
  analyze_variance_mean(data=import_data%>%filter(material == m))
  
}

```

There is a positive correlation between mean and variance for both materials. In Material1, there is a plausible quadratic fit for the mean-variance relationship, whereas in Material2, the plausibility of the quadratic fit diminishes, although there is a positive correlation visually. The ratio of variance and mean appears to vary across the data for both materials and reaches high values in a large proportion of rolling windows. 

In summary: 
1. Weekly usage distributions are right-tailed.
2. There is positive correlation between restock cycle length and total number of units used per cycle.
3. Usage is overdispersed within rolling windows.
4. Window usage mean is positively correlated with window usage variance.

This indicates that the **Poisson-Gamma conjugate model** can be used to model the weekly use.

The Prior the Prior for Gamma rate parameter is chosen as the median of the length of one inventory period, which is typically 7 days and the prior for Gamma shape parameter is chosen as the median of the weekly use / prior for Gamma rate.


# Negative Binomial Fit

Four functions are designed to perform the negative binomial fit:

1. **fit_nb_model**: This function computes the posterior parameters for the Poissonâ€“Gamma conjugate model, which leads to a Negative Binomial posterior predictive distribution.

Given weekly usage counts (y) and exposure times (n) and empirical Bayes priors (a0, b0) derived from the training window, the function returns: a = a0 + sum(y); b = b0 + sum(n)

These parameters define the posterior distribution of the weekly usage rate and will be used to generate predictive samples.

2. **predict_consumption_nb**: This function uses the posterior parameters from fit_nb_model to simulate posterior predictive outcomes for the total usage over the forecast horizon H_days.

It draws Monte Carlo samples from:

$Y_{future} \sim NegBinom(size = a,~ prob = \frac{b}{b ~+~ H_{days}})$

This distribution represents all plausible future usages consistent with past data.
The function returns: the median predictive consumption and the raw list of predictive samples (for use in decision-making)

3. **choose_restock_quantity**: This function transforms the predictive distribution into an operational decision.

For a grid of possible inventory stock levels $Q$, it computes the expected waste fraction:

$Expected~Waste~Fraction(Q) = \frac {E[max(Q ~-~ Y, ~0)]} {Q}$ , where $Y$ are the predictive samples.

The function identifies all $Q$ values where: 

$Expected~Waste~Fraction(Q) \leq Waste Threshold$

From these, it selects the maximum feasible stock level, i.e., the largest $Q$ that still keeps expected waste below the threshold.

This separates forecasting (uncertainty modeling) from decision-making (choosing stock under business constraints).

4. **cross_validate_nb**: This function performs rolling cross-validation across the time series:

At each time point, use a fixed training window.
i. Fit the Bayesian model.
ii. Generate the predictive distribution.
iii. Apply the decision rule to select optimal stock.
iv. Compare predicted vs. actual usage over the forecast horizon.
v. Store posteriors, predicted, actual, residuals.

This produces a full time-series evaluation of both predictive performance, decision performance and illustrates how forecasting behaves under real operational variability.


# Rolling-Origin Evaluation of Forecast

Rolling-origin evaluation is a standard validation technique used in time-series forecasting to obtain a series of forecasts over the validation set. Train window and test window sizes are chosen and multiple forecasts are performed along the validation set by moving through the chosen window lengths. Here, I have used this technique to assess the capacity of this modeling strategy to adapt to and capture varying shifts in the inventory data as we move along the windows.

## Material 1

### Rolling-Origin Evaluation

```{r, echo=TRUE}

cross_validate_material1_object <- cross_validate_nb(data=import_data%>%filter(material=='Material1'), H_days=50, train_window=8, step=1, Nmc=1000, level_weights=FALSE, waste_frac_max=0.15)

```

```{r}

cross_validate_material1 <- cross_validate_material1_object$results_df %>% mutate(row_id = row_number())

```

### Residual Plot

```{r}

head(cross_validate_material1)

patchwork::wrap_plots(
  ggplot(data=cross_validate_material1, aes(x=forecast_date, y=residual)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = 0), color="black") +
  geom_hline(aes(yintercept = 20), color="red") + 
  geom_hline(aes(yintercept = -20), color="red") +
  scale_x_date(date_breaks="8 weeks") +
  labs(title="Residual Plot for Material1") +
  theme_minimal(),

ggplot(data=cross_validate_material1, aes(x=var_mean_ratio, y=residual)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), color="black") +
  geom_hline(aes(yintercept = 20), color="red") + 
  geom_hline(aes(yintercept = -20), color="red") +
  labs(title="Residual vs Var-Mean Ratio for Material1") +
  theme_minimal(),

  nrow=2,
  ncol=1
)

```

We observe that the residuals highly variable and cross tolerable thresholds. There is also no apparent correlation between residual magnitude and window dispersion (variance-mean ratio).


### Posterior Plots for high residual predictions

```{r}

high_residual_rows_material1 <- sample(x=(cross_validate_material1%>%filter(abs(residual) > 20))$row_id, size=4, replace=FALSE)

```


```{r}

high_residual_plots_material1 <- lapply(high_residual_rows_material1, function(i) {
  
  plot_df <- data.frame(posterior=cross_validate_material1_object$posteriors[[i]])
  predicted_value <- cross_validate_material1$prediction[i]
  actual_value <- cross_validate_material1$actual_consumed[i]
  
  p <- ggplot(data=plot_df) +
    geom_density(aes(x=posterior), fill="grey", alpha=0.4) +
    geom_textvline(aes(xintercept=actual_value), color="blue", label="actual") +
    geom_textvline(aes(xintercept=predicted_value), color="red", label="predicted") +
    theme_minimal()
  
  return(p)
  
})



```


```{r, warning=FALSE, message=FALSE}

wrap_plots(high_residual_plots_material1) + plot_annotation(title="High Residual Windows - Posterior Distribution for Horizon")

```

Randomly sampled posteriors of high residual predictions show that the actual values typically fall in the tail of the posterior indicating that the Negative Binomial posterior alone could not explain the consumption behavior in those windows.

## Material 2

### Rolling-Origin Evaluation

```{r, echo=TRUE}

cross_validate_material2_object <- cross_validate_nb(import_data%>%filter(material=='Material2'), H_days=50, train_window=8, step=1, Nmc=1000, level_weights=TRUE, waste_frac_max=0.15)

```

```{r}

cross_validate_material2 <- cross_validate_material2_object$results_df %>% mutate(row_id = row_number())

```

### Residual Plots

```{r}

head(cross_validate_material2)

patchwork::wrap_plots(
  ggplot(data=cross_validate_material2, aes(x=forecast_date, y=residual)) +
  geom_line() +
  geom_point() +
  geom_hline(aes(yintercept = 0), color="black") +
  geom_hline(aes(yintercept = 7), color="red") + 
  geom_hline(aes(yintercept = -7), color="red") +
  scale_x_date(date_breaks="8 weeks") +
  labs(title="Residual Plot for Material2") +
  theme_minimal(),

ggplot(data=cross_validate_material2, aes(x=var_mean_ratio, y=residual)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), color="black") +
  geom_hline(aes(yintercept = 7), color="red") + 
  geom_hline(aes(yintercept = -7), color="red") +
  labs(title="Residual vs Var-Mean Ratio for Material2") +
  theme_minimal(),
  
  nrow=2,
  ncol=1
)

```

We observe that the residuals highly variable and cross tolerable thresholds. There is also no apparent correlation between residual magnitude and window dispersion (variance-mean ratio).

### Posterior Plots for high residual predictions

```{r}

high_residual_rows_material2 <- sample(x=(cross_validate_material2%>%filter(abs(residual) > 7))$row_id, size=4, replace=FALSE)

```


```{r}

high_residual_plots_material2 <- lapply(high_residual_rows_material2, function(i) {
  
  plot_df <- data.frame(posterior=cross_validate_material2_object$posteriors[[i]])
  predicted_value <- cross_validate_material2$prediction[i]
  actual_value <- cross_validate_material2$actual_consumed[i]
  
  p <- ggplot(data=plot_df) +
    geom_density(aes(x=posterior), fill="grey", alpha=0.4) +
    geomtextpath::geom_textvline(aes(xintercept=actual_value), color="blue", label="actual") +
    geomtextpath::geom_textvline(aes(xintercept=predicted_value), color="red", label="predicted") +
    theme_minimal()
  
  return(p)
  
})



```


```{r, warning=FALSE, message=FALSE}

patchwork::wrap_plots(high_residual_plots_material2) + plot_annotation(title="High Residual Windows - Posterior Distribution for Horizon")

```

Randomly sampled posteriors of high residual points show that the actual values can lie within the same quantile of the posterior, however, the tolerable threshold for this Material is too narrow compared to the distribution of residuals. There is also a fair chance for the actual value to lie on the tail of the posterior.

# Lessons Learned

The Bayesian Analysis did not produce stable decision-grade forecasts under these constraints. The analysis shows that the inherent variability in usage is not  well captured by a single stochastic process defined by fixed priors and likelihood, even with an overdispersed posterior such as negative binomial. This analysis revealed extent of complexity in the consumption process.

The critical insight is that **some processes are more suited to be managed primarily using human oversight with timely communication** rather than pure quantitative analysis.